---
title: "R Notebook"
output: html_notebook
---

## Clear the Workspace

```{r}
rm(list=ls())

```

## Load the Required Packages

```{r message=FALSE,warning=FALSE}
library(tm)
library(RTextTools)
library(stringr)
library(SnowballC)
library(ggplot2)
library(knitr)
library(tidyr)
```

## Load the Data

We are going to look at spam data from from [http://spamassassin.apache.org/old/](http://spamassassin.apache.org/old/).  Given that the sample that we are using isn't all that big, we'll try using Vcorpus.  We'll load the "spam" and "ham" samples into variables, and then add meta-data denoting which is which.

```{r}

basePath <- "C:/Users/Paul/OneDrive - CUNY School of Professional Studies/CUNY/DATA 607/Project 4/"

spam <- VCorpus(DirSource(paste(basePath,"spam/",sep=""),encoding="UTF-8"),
               readerControl = list(reader=readPlain))

ham <- VCorpus(DirSource(paste(basePath,"easy_ham/",sep=""),encoding="UTF-8"),
              readerControl = list(reader=readPlain))

meta(spam, tag = "is.Spam") <- "spam"
meta(ham, tag = "is.Spam") <- "not_spam"

```

## Prepare the Data

Next we'll combine the the data into a single VCorpus and clean it up a bit in preparation for conversion to a DocTermMatrix.  At this stage, I'm not going to perform any other cleaning/prep of the data because I'd like to assess the impact of it.

```{r}


ham.nSpam <- c(spam,ham,recursive=TRUE)

ham.nSpam <- sample(ham.nSpam,3000)

ham.nSpam <- tm_map(ham.nSpam,content_transformer(function(x) iconv(enc2utf8(x), sub = "byte")))
```

Now we'll create the DTM.  Originally I had intended to run with the data as raw as possible for this initial attempt, however, some cleaning (as above) was required in order to get the DTM to create properly.  As a side note, it appears that one can use a DTM or a TDM with similar results, so long as you make sure that the dimensions are correct when creating the "container" in the next step. 

```{r}
spam.dtm <- DocumentTermMatrix(ham.nSpam)
spam.dtm

#this works too, but you need to watch the dims when creating the container below.
#spam.tdm <- TermDocumentMatrix(ham.nSpam)
#spam.tdm

```

We can see above that the maximal term length is 515, which seems unrealistically high.  We'll explor that later - right now we'll create a vector with the true labels for training purposes.

```{r}

spam.label <- unlist(meta(ham.nSpam, "is.Spam")[,1])
head(spam.label,5)

```

## Train the Model(s)

Now we'll create the container that will be used to hold our training data and parameters.  

```{r}

N <- length(spam.label)
pct <-0.25
r <- round(N*pct,0)


container <- create_container(
  spam.dtm,
  labels = spam.label,
  trainSize = 1:r,
  testSize = (r+1):N,
  virgin=FALSE)

```

Next, we'll train on the training set and then classify the test set. I'm going to start with just SVM as the point here it to demonstrate the capability.  Then we'll look at a few more things afterwards. 

```{r}

#Train
svm.model <- train_model(container, "SVM")
#rf_model <- train_model(container, "RF")
#maxent_model <- train_model(container, "MAXENT")

#model <- train_model(container, algorithm="RF") 

#Classifying data
svm.out <- classify_model(container, svm.model)
#rf_out <- classify_model(container, rf_model)
#maxent_out <- classify_model(container, maxent_model)

#result <- classify_model(container, svm_model)

kable(head(svm.out,10))
```

And finally, we check to see how accurate the models predictions were:

```{r}

true.labels <- as.numeric(as.factor(spam.label[(r+1):N]))
predicted.labels <- svm.out[,"SVM_LABEL"]
recall_accuracy(true.labels,predicted.labels)

```


## Cleaning Up the Data

We'll now take a look at what happens if we run the same analysis on data that we've cleaned.  Normally I would be tempted to determine our the sensitivity of our results to each element of the cleaning, but I think that's beyond the scope of this project.  In this case, I'm going to clean it all in one shot.  We'll look make the following changes:

* Remove the numbers
* Remove the punctuation
* Remove the stop-words
* Make the data uniformly lowercase
* Apply stemming
* Remove Sparse Terms

```{r}

clean.spam <- ham.nSpam

clean.spam <- tm_map(clean.spam,content_transformer(tolower),lazy=TRUE)

clean.spam <- tm_map(clean.spam,removePunctuation)
clean.spam <- tm_map(clean.spam,removeNumbers)
clean.spam <- tm_map(clean.spam,removeWords,words=stopwords("en"))
clean.spam <- tm_map(clean.spam,stripWhitespace,lazy=TRUE)
clean.spam <- tm_map(clean.spam,stemDocument,lazy=TRUE)

clean.spam <- tm_map(clean.spam ,content_transformer(function(x) iconv(enc2utf8(x), sub = "byte")))



clean.dtm <- DocumentTermMatrix(clean.spam)

clean.dtm <- removeSparseTerms(clean.dtm,0.95)

clean.dtm
```

We've cleaned the data and reduced the sparsity of the DTM a little bit.  The maximal term length is down by about 90% from the uncleaned data.  Now We'll check which produces better results.  We'll write a little function to train the model and assess the output on a random sample of the data, and then run a few iterations:


```{r}

modelTest <- function(sampleSize, test_prop, corpus){
  r<-round(sampleSize*test_prop)

  
  data <- sample(corpus,sampleSize)
  labels <- unlist(meta(data, "is.Spam")[,1])
  dtm <- DocumentTermMatrix(data)
  
  container <- create_container(
  dtm,
  labels = labels,
  trainSize = 1:r,
  testSize = (r+1):sampleSize,
  virgin=FALSE) 
  
  svm.model <- train_model(container, "SVM")
  svm.out <- classify_model(container, svm.model)
  
  true.labels <- as.numeric(as.factor(labels[(r+1):sampleSize]))
  predicted.labels <- svm.out[,"SVM_LABEL"]
  out <- recall_accuracy(true.labels,predicted.labels)
  return(out)
  
}
```


```{r}
itr <-25
orig <- rep(0,itr)
clean <- rep(0,itr)


for (i in 1:itr){
  print(i)
  orig[i] <- modelTest(100,0.25,ham.nSpam) 
  clean[i] <- modelTest(100,0.25,clean.spam) 
  
}


```

The above is painfully slow and i assume that there is a much better way to accomplish similar this task.  However, in this case, we've gotten the desired data.

```{r}


data <- data.frame(orig=orig,clean=clean)

summary(data)

tidy.data <- gather(data)

ggplot(tidy.data,aes(x=value,fill=key)) + geom_histogram(alpha=0.75,bins=10)

```

For this particular task, the original (uncleaned) doesn't appear to produce results that are significantly different from the "clean" data, at least on an SVM model.




```{r}

#Train
multi.model <- train_models(container, algorithm=c("SVM","RF","MAXENT"))

multi.out <- classify_models(container, multi.model)

kable(head(multi.out,10))

```

```{r}

true.labels <- as.numeric(as.factor(spam.label[(r+1):N]))
predicted.labels <- svm.out[,"SVM_LABEL"]
svm <- recall_accuracy(true.labels,predicted.labels)

predicted.labels <- svm.out[,"FORESTS_LABEL"]
rf <- recall_accuracy(true.labels,predicted.labels)

predicted.labels <- svm.out[,"MAXENTROPY_LABEL"]
maxent <- recall_accuracy(true.labels,predicted.labels)

results <- data.frame(svm=svm,rf=rf,maxent=maxent)

tidy.results <- gather(results)

ggplot(tidy.results,aes(x=key, y=value,fill=key)) + 
  geom_bar(stat="identity") +
  coord_cartesian(ylim = c(0.975, 1.00))+
  xlab("Model")+
  ylab("Success Rate")+
  ggtitle("Prediction Success Rate for Various Methods")+
  theme(plot.title = element_text(hjust = 0.5))

```